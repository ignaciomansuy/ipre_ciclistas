{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.12.3 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3050, 8192MiB)\n",
      "Setup complete  (32 CPUs, 31.8 GB RAM, 479.6/930.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervision.__version__: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install supervision\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import supervision as sv\n",
    "print(\"supervision.__version__:\", sv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"yolov8x.pt\"\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(MODEL)\n",
    "model.fuse()\n",
    "# dict maping class_id to class_name\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "\n",
    "# class_ids of interest - car, motorcycle, bus and truck\n",
    "selected_classes = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from IPython import display\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VARIABLES2\n",
    "import importlib\n",
    "importlib.reload(VARIABLES2)\n",
    "from VARIABLES2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hypotenuse(a, b):\n",
    "  return math.sqrt(a**2 + b**2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInfoHandler():\n",
    "  def __init__(self) -> None:\n",
    "    self.video_info = None\n",
    "    self.va_params = {}\n",
    "    self.line_zone_annotators = []\n",
    "    self.label_annotator = None\n",
    "    self.trace_annotator = None\n",
    "    self.byte_tracker = None\n",
    "    self.line_zones = [] \n",
    "    pass\n",
    "    \n",
    "  def re_init(self, SOURCE_VIDEO_PATH):\n",
    "    self.video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "    self.init_va_params()\n",
    "    self.init_line_zone_annotators()\n",
    "    self.label_annotator = sv.LabelAnnotator( text_thickness=self.va_params[\"text_thickness\"], text_scale=self.va_params[\"text_scale\"])\n",
    "    self.trace_annotator = sv.TraceAnnotator(thickness=self.va_params[\"thickness\"], trace_length=self.va_params[\"trace_length\"])\n",
    "    self.byte_tracker = sv.ByteTrack(\n",
    "      track_activation_threshold=0.25, lost_track_buffer=30, minimum_matching_threshold=0.8, frame_rate=self.video_info.fps\n",
    "    )\n",
    "    self.init_line_zones()\n",
    "    \n",
    "  def init_va_params(self):\n",
    "    video_default_size = calculate_hypotenuse(1920, 1080)\n",
    "    video_current_size = calculate_hypotenuse(self.video_info.width, self.video_info.height)\n",
    "    proportion = video_current_size / video_default_size\n",
    "    self.va_params = {\n",
    "        \"thickness\": round(THICKNESS_DEFAULT * proportion),\n",
    "        \"text_thickness\": round(TEXT_THICKNESS_DEFAULT * proportion),\n",
    "        \"text_scale\": TEXT_SCALE_DEFAULT * proportion,\n",
    "        \"trace_length\": round(TRACE_LENGTH_DEFAULT * proportion),\n",
    "    }\n",
    "    \n",
    "  def init_line_zone_annotators(self):\n",
    "    self.line_zone_annotators = [sv.LineZoneAnnotator(\n",
    "                thickness=self.va_params[\"thickness\"],\n",
    "                text_thickness=self.va_params[\"text_thickness\"],\n",
    "                text_scale=self.va_params[\"text_scale\"]\n",
    "                )\n",
    "              for _ in range(3)]\n",
    "    \n",
    "  def get_line_zones(self):\n",
    "    line_zones = []\n",
    "    for i in [-1, 0, 1]:\n",
    "        x = self.video_info.width * (1 / 2 + i * 0.15)\n",
    "        line_zones.append(\n",
    "          sv.LineZone(\n",
    "          start=sv.Point( x, 0),\n",
    "          end=sv.Point(x, self.video_info.height)\n",
    "          )\n",
    "        )\n",
    "    return line_zones\n",
    "          \n",
    "  def init_line_zones(self):\n",
    "    new_line_zones = self.get_line_zones()\n",
    "    if self.line_zones:\n",
    "      for i, ex_line in enumerate(self.line_zones):\n",
    "        new_line_zones[i].in_count = ex_line.in_count\n",
    "        new_line_zones[i].out_count = ex_line.out_count\n",
    "    \n",
    "    self.line_zones = new_line_zones\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video(\n",
    "    source_path: str,\n",
    "    target_path: str,\n",
    "    callback,\n",
    "    stride=1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process a video file by applying a callback function on each frame\n",
    "        and saving the result to a target video file.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the source video file.\n",
    "        target_path (str): The path to the target video file.\n",
    "        callback (Callable[[np.ndarray, int], np.ndarray]): A function that takes in\n",
    "            a numpy ndarray representation of a video frame and an\n",
    "            int index of the frame and returns a processed numpy ndarray\n",
    "            representation of the frame.\n",
    "\n",
    "    Examples:\n",
    "        ```python\n",
    "        import supervision as sv\n",
    "\n",
    "        def callback(scene: np.ndarray, index: int) -> np.ndarray:\n",
    "            ...\n",
    "\n",
    "        process_video(\n",
    "            source_path=<SOURCE_VIDEO_PATH>,\n",
    "            target_path=<TARGET_VIDEO_PATH>,\n",
    "            callback=callback\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    source_video_info = sv.VideoInfo.from_video_path(video_path=source_path)\n",
    "    with sv.VideoSink(target_path=target_path, video_info=source_video_info) as sink:\n",
    "        for index, frame in tqdm(enumerate(\n",
    "            sv.get_video_frames_generator(source_path=source_path, stride=stride)\n",
    "        ), desc=\" Video processing\", position=1, leave=False, total=source_video_info.total_frames):\n",
    "            result_frame = callback(frame, index)\n",
    "            sink.write_frame(frame=result_frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a043727e702c4accbe7f5847b8cd7f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Videos remaining:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6869fe48a25e48758c09805816ec7f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Video processing:   0%|          | 0/18032 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccb86a424d849ef9b87446a4a007621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Video processing:   0%|          | 0/18031 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5017c7d140341e392b56181d35f0427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Video processing:   0%|          | 0/18032 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f39955ef0cb41f98cc2fc1d90d4ac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Video processing:   0%|          | 0/18031 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37829074f654a67ac6ad0c76c1b947c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Video processing:   0%|          | 0/2682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "153 114\n"
     ]
    }
   ],
   "source": [
    "import VARIABLES2\n",
    "import importlib\n",
    "importlib.reload(VARIABLES2)\n",
    "from VARIABLES2 import *\n",
    "\n",
    "\n",
    "vih = VideoInfoHandler()\n",
    "\n",
    "def callback(frame: np.ndarray, index:int) -> np.ndarray:\n",
    "    # model prediction on single frame and conversion to supervision Detections\n",
    "    results = model(frame, verbose=False, device=torch.device(\"cuda:0\"))[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    # only consider class id from selected_classes define above \n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)]\n",
    "    # tracking detections\n",
    "    detections = vih.byte_tracker.update_with_detections(detections)\n",
    "    labels = [\n",
    "        f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
    "        for confidence, class_id, tracker_id\n",
    "        in zip(detections.confidence, detections.class_id, detections.tracker_id)\n",
    "    ]\n",
    "    annotated_frame = vih.trace_annotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections\n",
    "    )\n",
    "    annotated_frame=vih.label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        labels=labels)\n",
    "\n",
    "    # update line counter\n",
    "    for line_zone in vih.line_zones:\n",
    "        line_zone.trigger(detections)\n",
    "    # return frame with box and line annotated result\n",
    "    for i in range(3):\n",
    "        annotated_frame = vih.line_zone_annotators[i].annotate(annotated_frame, line_counter=vih.line_zones[i])\n",
    "    return  annotated_frame\n",
    "\n",
    "\n",
    "videos_folder = \"3-3\"\n",
    "videos_folder_path = os.path.join(\"full_recordings\", videos_folder)\n",
    "\n",
    "\n",
    "data = [[\"file_name\", \"in\", \"out\"]]\n",
    "\n",
    "prev_in, prev_out = 0,  0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(\"results\", f\"{videos_folder}.csv\"), \"w\", newline=\"\") as csv_output:\n",
    "  writer = csv.writer(csv_output)\n",
    "  try:\n",
    "    for file_name in tqdm(os.listdir(videos_folder_path), desc=\" Videos remaining\", position=0):\n",
    "      file_path = os.path.join(videos_folder_path, file_name)\n",
    "      vih.re_init(file_path)\n",
    "      process_video(\n",
    "        source_path = file_path,\n",
    "        target_path = TARGET_DUMMY_VIDEO_PATH,\n",
    "        callback=callback,\n",
    "      )\n",
    "      max_in = max(vih.line_zones, key=lambda x: x.in_count)\n",
    "      max_out = max(vih.line_zones, key=lambda x: x.out_count)\n",
    "      \n",
    "      data.append([file_name, max_in.in_count - prev_in, max_out.out_count - prev_out])\n",
    "      prev_in = max_in.in_count\n",
    "      prev_out = max_out.out_count\n",
    "      \n",
    "  except KeyError as e:\n",
    "    print(e)\n",
    "  finally: \n",
    "    writer.writerows(data)\n",
    "    result = subprocess.run(f\"echo {max_in.in_count}, {max_out.out_count} > results/{videos_folder}.txt\", shell=True, capture_output=True, text=True)\n",
    "\n",
    "    # Check if the command was successful\n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Command failed with error:\")\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "    print(max_in.in_count, max_out.out_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
